{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ddqn = DDQN(latent_dim, action_dim).to(device)\n",
    "# optimizer = torch.optim.Adam(ddqn.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for (state, action, reward, next_state, done) in replay_buffer:\n",
    "#         state = vae.encoder(state).detach()\n",
    "#         next_state = vae.encoder(next_state).detach()\n",
    "#         q_values = ddqn(state)\n",
    "#         next_q_values = ddqn(next_state)\n",
    "#         target = reward + gamma * torch.max(next_q_values, dim=1)[0] * (1 - done)\n",
    "#         loss = F.mse_loss(q_values.gather(1, action.unsqueeze(1)), target.unsqueeze(1))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z), z\n",
    "\n",
    "\n",
    "\n",
    "input_dim = len()\n",
    "latent_dim = 5\n",
    "num_epochs = 5000\n",
    "batch_size = 100\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr=1e-3\n",
    "num_episodes = num_epochs\n",
    "initial_lr = lr\n",
    "final_lr = lr*1e-3\n",
    "n_updates = num_episodes*0.7\n",
    "lr_gamma =  (final_lr / initial_lr)**(1 / n_updates) # gamma\n",
    "\n",
    "vae = VAE(input_dim, latent_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=n_updates, gamma=lr_gamma)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming data is your 2x10000 numpy array\n",
    "data = np.random.rand(2, 10000)\n",
    "\n",
    "# get data from checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "data_tensor = torch.from_numpy(data)\n",
    "\n",
    "# Create a TensorDataset from the tensor\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Create a DataLoader from the TensorDataset\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for (inputs, _) in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        outputs, _ = vae(inputs)\n",
    "        loss = F.binary_cross_entropy(outputs, inputs, reduction='sum')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
