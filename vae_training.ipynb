{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<shelve.DbfilenameShelf object at 0x7fe1112e3820>\n",
      "['Agent', 'AgentClass', 'CommunicationP3DX', 'Empty', 'Memory', 'QNetwork', 'QNetworkCNN', 'Q_1', 'StepLR', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', '__warningregistry__', 'actions_angular', 'actions_linear', 'checkpoint_inter', 'datetime', 'device', 'evaluate', 'filename', 'goal_zones_x', 'goal_zones_y', 'hist_dict', 'key', 'laser_scan_state_type_atual', 'main', 'max_episodes', 'max_iterations', 'my_shelf', 'n_sectors', 'path', 'pause_physics_client', 'performance', 'reset_simulation', 'select_action', 'str_hora_agr', 'str_hora_inicio_treino', 'theta_atual', 'train', 'unpause_physics_client', 'update_parameters']\n"
     ]
    }
   ],
   "source": [
    "# ddqn = DDQN(latent_dim, action_dim).to(device)\n",
    "# optimizer = torch.optim.Adam(ddqn.parameters(), lr=learning_rate)\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     for (state, action, reward, next_state, done) in replay_buffer:\n",
    "#         state = vae.encoder(state).detach()\n",
    "#         next_state = vae.encoder(next_state).detach()\n",
    "#         q_values = ddqn(state)\n",
    "#         next_q_values = ddqn(next_state)\n",
    "#         target = reward + gamma * torch.max(next_q_values, dim=1)[0] * (1 - done)\n",
    "#         loss = F.mse_loss(q_values.gather(1, action.unsqueeze(1)), target.unsqueeze(1))\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from CommunicationP3DX import CommunicationP3DX\n",
    "from Agent import AgentClass\n",
    "from std_srvs.srv import Empty\n",
    "import rospy\n",
    "import time\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "path ='/media/xnd/7A309A87309A49D1/sia_23/10k/sim_data/'\n",
    "\n",
    "workspace = shelve.open(path+'wsh_mean4_20231001_183307/wsh_20231003_042932.out')\n",
    "# true if the key exists\n",
    "vars = list(workspace.keys())\n",
    "print(workspace)\n",
    "print(vars)\n",
    "hist_dict = workspace['hist_dict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, latent_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, latent_dim)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 500),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(500, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z), z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "random_ep_idx = 0\n",
      "random_ep_idx = 100\n",
      "random_ep_idx = 200\n",
      "random_ep_idx = 300\n",
      "random_ep_idx = 400\n",
      "random_ep_idx = 500\n",
      "random_ep_idx = 600\n",
      "random_ep_idx = 700\n",
      "random_ep_idx = 800\n",
      "random_ep_idx = 900\n"
     ]
    }
   ],
   "source": [
    "# creating dummy targets (float values)\n",
    "episode = 9900\n",
    "\n",
    "random_episodes =np.random.randint(2000,8000,1000)\n",
    "for random_ep_idx, episode in enumerate(random_episodes):\n",
    "    if random_ep_idx%100==0: print('random_ep_idx = {ep_id}'.format(ep_id =random_ep_idx))\n",
    "    # shape = (727, 16)\n",
    "    trasnposed_scans = np.asarray([np.asarray(scan_i) for scan_i in hist_dict['scan'][episode]]).transpose()    \n",
    "\n",
    "    if random_ep_idx==0:\n",
    "        df_episode_positions = pd.DataFrame({#'i':range(len(hist_dict['pos'][episode])),\n",
    "                                            'x':[pos[0] for pos in hist_dict['pos'][episode]], \n",
    "                                            'y':[pos[1] for pos in hist_dict['pos'][episode]]})\n",
    "\n",
    "        for scan_idx in range(0,727):\n",
    "            df_episode_positions['scan_'+str(scan_idx)] = pd.to_numeric([float(scan_i)  for scan_i in trasnposed_scans[scan_idx]])\n",
    "\n",
    "    else:\n",
    "        new_episode_positions = pd.DataFrame({'x':[pos[0] for pos in hist_dict['pos'][episode]], \n",
    "                                            'y':[pos[1] for pos in hist_dict['pos'][episode]]})\n",
    "        \n",
    "        for scan_idx in range(0,727):\n",
    "            new_episode_positions['scan_'+str(scan_idx)] = pd.to_numeric([float(scan_i)  for scan_i in trasnposed_scans[scan_idx]])\n",
    "\n",
    "        df_episode_positions = pd.concat([df_episode_positions,new_episode_positions])\n",
    "        \n",
    "\n",
    "    df_episode_positions = df_episode_positions.replace(np.Inf, np.NaN).fillna(5)\n",
    "\n",
    "df_episode_positions.to_pickle('dataset_1_vae.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-3.6178,  2.0000,  2.0961,  ...,  5.0000,  5.0000,  5.0000],\n",
      "        [-3.2115,  2.1438,  2.1437,  ...,  5.0000,  5.0000,  5.0000],\n",
      "        [-2.5263,  2.2658,  3.6359,  ...,  0.7862,  0.7772,  0.7708],\n",
      "        ...,\n",
      "        [-8.7927,  8.5720,  1.5012,  ...,  5.0000,  5.0000,  5.0000],\n",
      "        [-8.6550,  8.0715,  1.3688,  ...,  2.1308,  2.1094,  2.1062],\n",
      "        [-8.1130,  7.9758,  5.0000,  ...,  2.3749,  2.3763,  2.3588]],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# creating tensor from targets_df \n",
    "data = df_episode_positions.values # .to_numpy()\n",
    "torch_tensor = torch.tensor(data)\n",
    "\n",
    "# printing out result\n",
    "print(torch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1819959.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1780739.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1778248.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1758865.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1842617.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1851257.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1794730.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1854620.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1810173.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1785196.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1828045.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1775494.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1858457.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1844986.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1783885.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1770342.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1788129.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1765796.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1734397.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1809012.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1753108., grad_fn=<MseLossBackward0>)\n",
      "tensor(1818681.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1802178.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1781025.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1781053.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1796875.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1769071.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1763166.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1804436., grad_fn=<MseLossBackward0>)\n",
      "tensor(1834387.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1838148.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1801863.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1720177.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1795227.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1887603.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1773668.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1817867.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1847735.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1795974., grad_fn=<MseLossBackward0>)\n",
      "tensor(1860777.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1876306.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1789259.7500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1791713.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1826570.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1795099.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1850409.6250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1789124.3750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1749667.8750, grad_fn=<MseLossBackward0>)\n",
      "tensor(1803147.1250, grad_fn=<MseLossBackward0>)\n",
      "tensor(1826559.2500, grad_fn=<MseLossBackward0>)\n",
      "tensor(1868134.5000, grad_fn=<MseLossBackward0>)\n",
      "tensor(1785905., grad_fn=<MseLossBackward0>)\n",
      "tensor(1785429.8750, grad_fn=<MseLossBackward0>)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/xnd/Desktop/sia_DRL_2023/vae_training.ipynb Cell 5\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/vae_training.ipynb#W3sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m     loss \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mbinary_cross_entropy(outputs, inputs, reduction\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39msum\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/vae_training.ipynb#W3sZmlsZQ%3D%3D?line=35'>36</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/vae_training.ipynb#W3sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m     loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/vae_training.ipynb#W3sZmlsZQ%3D%3D?line=37'>38</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mstep()\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/vae_training.ipynb#W3sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m scheduler\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    478\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    479\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    480\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    485\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    486\u001b[0m     )\n\u001b[0;32m--> 487\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    488\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    489\u001b[0m )\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/autograd/__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    197\u001b[0m \u001b[39m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    198\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    199\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 200\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    201\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[1;32m    202\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "input_dim = 727+2 # 727 laser readings + 2 pos readings\n",
    "latent_dim = 5\n",
    "num_epochs = 5000\n",
    "batch_size = 500\n",
    "\n",
    "# Convert numpy array to PyTorch tensor\n",
    "data_tensor = torch.from_numpy(data)\n",
    "\n",
    "# Create a TensorDataset from the tensor\n",
    "dataset = TensorDataset(data_tensor)\n",
    "\n",
    "# Create a DataLoader from the TensorDataset\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "lr=1e-3\n",
    "num_episodes = num_epochs\n",
    "initial_lr = lr\n",
    "final_lr = lr*1e-2\n",
    "n_updates = num_episodes*0.7\n",
    "lr_gamma =  (final_lr / initial_lr)**(1 / n_updates) # gamma\n",
    "\n",
    "vae = VAE(input_dim, latent_dim).to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(vae.parameters(), lr=lr)\n",
    "scheduler = StepLR(optimizer, step_size=n_updates, gamma=lr_gamma)\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for (inputs,) in dataloader:\n",
    "        inputs = inputs.to(torch.float32).to(device)\n",
    "        outputs, _ = vae(inputs)\n",
    "        printloss =  F.mse_loss(outputs, inputs, reduction='sum') # \n",
    "        loss = F.binary_cross_entropy(outputs, inputs, reduction='sum')\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    scheduler.step()\n",
    "    if epoch%5==0:print(printloss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
