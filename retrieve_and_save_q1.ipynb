{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\"\"\"\n",
    "Adapted from https://github.com/fschur/DDQN-with-PyTorch-for-OpenAI-Gym/blob/master/DDQN_discrete.py#L21\n",
    "for a gazebo application.\n",
    "\n",
    "Implementation of Double DQN for gym environments with discrete action space.\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from CommunicationP3DX import CommunicationP3DX\n",
    "from Agent import AgentClass\n",
    "from std_srvs.srv import Empty\n",
    "import rospy\n",
    "import time\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "str_hora_inicio_treino = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "\n",
    "\"\"\"\n",
    "The Q-Network has as input a state s and outputs the state-action values q(s,a_1), ..., q(s,a_n) for all n actions.\n",
    "\"\"\"\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        x1 = F.leaky_relu(self.fc_1(inp))\n",
    "        x1 = F.leaky_relu(self.fc_2(x1))\n",
    "        x1 = self.fc_3(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If the observations are images we use CNNs.\n",
    "\"\"\"\n",
    "class QNetworkCNN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(QNetworkCNN, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, kernel_size=4, stride=3)\n",
    "        self.conv_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc_1 = nn.Linear(8960, 512)\n",
    "        self.fc_2 = nn.Linear(512, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view((1, 3, 210, 160))\n",
    "        x1 = F.relu(self.conv_1(inp))\n",
    "        x1 = F.relu(self.conv_2(x1))\n",
    "        x1 = F.relu(self.conv_3(x1))\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x1 = F.leaky_relu(self.fc_1(x1))\n",
    "        x1 = self.fc_2(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "memory to save the state, action, reward sequence from the current episode.\n",
    "\"\"\"\n",
    "class Memory:\n",
    "    def __init__(self, len):\n",
    "        self.rewards = collections.deque(maxlen=len)\n",
    "        self.state = collections.deque(maxlen=len)\n",
    "        self.action = collections.deque(maxlen=len)\n",
    "        self.is_done = collections.deque(maxlen=len)\n",
    "\n",
    "    def update(self, state, action, reward, done):\n",
    "        # if the episode is finished we do not save to new state. Otherwise we have more states per episode than rewards\n",
    "        # and actions whcih leads to a mismatch when we sample from memory.\n",
    "        if not done:\n",
    "            self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.is_done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        sample \"batch_size\" many (state, action, reward, next state, is_done) datapoints.\n",
    "        \"\"\"\n",
    "        n = len(self.is_done)\n",
    "        idx = random.sample(range(0, n-1), batch_size)\n",
    "\n",
    "        return torch.Tensor(self.state)[idx].to(device), torch.LongTensor(self.action)[idx].to(device), \\\n",
    "               torch.Tensor(self.state)[1+np.array(idx)].to(device), torch.Tensor(self.rewards)[idx].to(device), \\\n",
    "               torch.Tensor(self.is_done)[idx].to(device)\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards.clear()\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.is_done.clear()\n",
    "\n",
    "\n",
    "def select_action(model, action_space_size, state, eps):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    with torch.no_grad():\n",
    "        values = model(state)\n",
    "\n",
    "    # select a random action wih probability eps\n",
    "    if random.random() <= eps:\n",
    "        action = np.random.randint(0, action_space_size)\n",
    "    else:\n",
    "        action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def train(batch_size, current, target, optim, memory, gamma):\n",
    "\n",
    "    states, actions, next_states, rewards, is_done = memory.sample(batch_size)\n",
    "\n",
    "    q_values = current(states)\n",
    "\n",
    "    next_q_values = current(next_states)\n",
    "    next_q_state_values = target(next_states)\n",
    "\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = rewards + gamma * next_q_value * (1 - is_done)\n",
    "\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "def evaluate(Qmodel, horizon_eval, repeats,episode):\n",
    "    \"\"\"\n",
    "    Runs a greedy policy with respect to the current Q-Network for \"repeats\" many episodes. Returns the average\n",
    "    episode reward.\n",
    "    \"\"\"\n",
    "    Qmodel.eval()\n",
    "    perform = 0\n",
    "    for repeat_i in range(repeats):\n",
    "\n",
    "        #env.reset()\n",
    "        state = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "        reset_simulation()\n",
    "\n",
    "        done = False\n",
    "        i= 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                values = Qmodel(state)\n",
    "            action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "            if action==0:\n",
    "                Com.action_forward()\n",
    "            elif action==1:\n",
    "                Com.action_right()\n",
    "            else:\n",
    "                Com.action_left()\n",
    "\n",
    "            state=Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "            done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon_eval, reach_dist=0.5)\n",
    "\n",
    "            if i > horizon_eval:\n",
    "                done = True\n",
    "\n",
    "        if repeat_i==0:\n",
    "            hist_dict['rewards'][episode+1] = [[reward, new_distance, r1, r4]]\n",
    "            hist_dict['epresult'][episode+1] = [done, status_done]\n",
    "\n",
    "            hist_dict['pos_eval'][episode+1] = [Agent.Pos]\n",
    "            hist_dict['scan_eval'][episode+1] = [Agent.laser_scan]\n",
    "            hist_dict['state_eval'][episode+1] = [state]\n",
    "        else:\n",
    "            hist_dict['rewards'][episode+1].append([reward, new_distance, r1, r4])\n",
    "            hist_dict['epresult'][episode+1].append([done, status_done])\n",
    "\n",
    "            hist_dict['pos_eval'][episode+1].append(Agent.Pos)\n",
    "            hist_dict['scan_eval'][episode+1].append(Agent.laser_scan)\n",
    "            hist_dict['state_eval'][episode+1].append(state)\n",
    "\n",
    "\n",
    "            perform += reward\n",
    "    Qmodel.train()\n",
    "    return perform/repeats, new_distance, r1, r4\n",
    "\n",
    "\n",
    "def update_parameters(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "\n",
    "def main(state_space_size, action_space_size=3, gamma=0.99, lr=1e-3, min_episodes=20, eps=1, eps_decay=0.995,\n",
    "         eps_min=0.01, update_step=10, batch_size=64, update_repeats=50, num_episodes=3000, max_memory_size=50000,\n",
    "         lr_gamma=0.9, lr_step=100, measure_step=100, measure_repeats=100, hidden_dim=64, cnn=False, horizon=200000,horizon_eval=50,\n",
    "         theta_atual:int= 36,laser_scan_state_type_atual:str = 'min',checkpoint_inter=1000):\n",
    "    \"\"\"\n",
    "    :param gamma: reward discount factor\n",
    "    :param lr: learning rate for the Q-Network\n",
    "    :param min_episodes: we wait \"min_episodes\" many episodes in order to aggregate enough data before starting to train\n",
    "    :param eps: probability to take a random action during training\n",
    "    :param eps_decay: after every episode \"eps\" is multiplied by \"eps_decay\" to reduces exploration over time\n",
    "    :param eps_min: minimal value of \"eps\"\n",
    "    :param update_step: after \"update_step\" many episodes the Q-Network is trained \"update_repeats\" many times with a\n",
    "    batch of size \"batch_size\" from the memory.\n",
    "    :param batch_size: see above\n",
    "    :param update_repeats: see above\n",
    "    :param num_episodes: the number of episodes played in total\n",
    "    :param seed: random seed for reproducibility\n",
    "    :param max_memory_size: size of the replay memory\n",
    "    :param lr_gamma: learning rate decay for the Q-Network\n",
    "    :param lr_step: every \"lr_step\" episodes we decay the learning rate\n",
    "    :param measure_step: every \"measure_step\" episode the performance is measured\n",
    "    :param measure_repeats: the amount of episodes played in to asses performance\n",
    "    :param hidden_dim: hidden dimensions for the Q_network\n",
    "    :param env_name: name of the gym environment\n",
    "    :param cnn: set to \"True\" when using environments with image observations like \"Pong-v0\"\n",
    "    :param horizon: number of steps taken in the environment before terminating the episode (prevents very long episodes)\n",
    "    :param render: if \"True\" renders the environment every \"render_step\" episodes\n",
    "    :param render_step: see above\n",
    "    :return: the trained Q-Network and the measured performances\n",
    "    \"\"\"\n",
    "\n",
    "    if cnn:\n",
    "        Q_1 = QNetworkCNN(action_dim=action_space_size).to(device)\n",
    "        Q_2 = QNetworkCNN(action_dim=action_space_size).to(device)\n",
    "    else:\n",
    "        Q_1 = QNetwork(action_dim=action_space_size, state_dim=state_space_size,\n",
    "                                        hidden_dim=hidden_dim).to(device)\n",
    "        Q_2 = QNetwork(action_dim=action_space_size, state_dim=state_space_size,\n",
    "                                        hidden_dim=hidden_dim).to(device)\n",
    "    # transfer parameters from Q_1 to Q_2\n",
    "    update_parameters(Q_1, Q_2)\n",
    "\n",
    "    # we only train Q_1\n",
    "    for param in Q_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # https://douglasrizzo.com.br/step-decay-lr/\n",
    "    initial_lr = lr\n",
    "    final_lr = lr*1e-3\n",
    "    n_updates = num_episodes*0.7\n",
    "    lr_gamma =  (final_lr / initial_lr)**(1 / n_updates) # gamma\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q_1.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "\n",
    "    memory = Memory(max_memory_size)\n",
    "    performance = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # display the performance\n",
    "        if (episode % measure_step == 0) and episode >= min_episodes:\n",
    "            print('eval...')\n",
    "            reward_eval, new_distance, r1, r4 = evaluate(Q_1, horizon_eval, measure_repeats,episode)\n",
    "            performance.append([episode, reward_eval])\n",
    "            print(\"Episode: \", episode)\n",
    "            print(\"rewards: \", performance[-1][1])\n",
    "            print(\"lr: \", scheduler.get_lr()[0])\n",
    "            print(\"eps: \", eps)\n",
    "\n",
    "            # hist_dict = {'pos':{}, 'scan':{}, 'rewards':{}, 'rates':{}, 'state':{}, 'epresult':{}}\n",
    "            hist_dict['rewards_eval'][episode+1] = [performance[-1][1], new_distance, r1, r4]\n",
    "            hist_dict['rates'][episode+1] = [eps, scheduler.get_lr()[0]]\n",
    "\n",
    "        reset_simulation() #env.reset()\n",
    "        Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "        state = Agent.state\n",
    "        #print(state)\n",
    "        memory.state.append(state)\n",
    "\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "            #if state is None: break\n",
    "            action = select_action(Q_2, action_space_size, state, eps)\n",
    "\n",
    "            #TODO take action here, calculate rewards, check if done, refresh state variable\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if action==0:\n",
    "                Com.action_forward()\n",
    "            elif action==1:\n",
    "                Com.action_right()\n",
    "            else:\n",
    "                Com.action_left()\n",
    "\n",
    "\n",
    "            state  = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            #print(state)\n",
    "            #reward, _, _, _ = Agent.get_reward(number_iterations=i)\n",
    "            reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "            done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon, reach_dist=0.5)\n",
    "\n",
    "\n",
    "            # hist_dict = {'pos':{}, 'scan':{}, 'rewards':{}, 'rates':{}, 'state':{}, 'epresult':{}}\n",
    "            if i==1:\n",
    "                hist_dict['pos'][episode+1] = [Agent.Pos]\n",
    "                hist_dict['scan'][episode+1] = [Agent.laser_scan]\n",
    "                hist_dict['state'][episode+1] = [state]\n",
    "            else:\n",
    "                hist_dict['pos'][episode+1].append(Agent.Pos)\n",
    "                hist_dict['scan'][episode+1].append(Agent.laser_scan)\n",
    "                hist_dict['state'][episode+1].append(state)\n",
    "\n",
    "            if i > horizon:\n",
    "                done = True\n",
    "\n",
    "            if (episode % (measure_step) == 0) and i%5==0:\n",
    "                if i<=5:\n",
    "                    str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "                    print('Epsisode '+str(episode)+' @'+str_hora_agr)\n",
    "\n",
    "                str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "                print('i: '+str(i)+' \\tr:'+str([reward, new_distance, r1, r4]))\n",
    "\n",
    "            # save state, action, reward sequence\n",
    "            memory.update(state, action, reward, done)\n",
    "\n",
    "\n",
    "        if episode >= min_episodes and episode % update_step == 0:\n",
    "            for _ in range(update_repeats):\n",
    "\n",
    "                pause_physics_client() # pause simulation to train\n",
    "                train(batch_size, Q_1, Q_2, optimizer, memory, gamma)\n",
    "                unpause_physics_client() # unpause simulation after training\n",
    "                reset_simulation()\n",
    "\n",
    "            # transfer new parameter from Q_1 to Q_2\n",
    "            update_parameters(Q_1, Q_2)\n",
    "            # print(Agent.goal)\n",
    "\n",
    "        # update learning rate and eps\n",
    "        scheduler.step()\n",
    "        eps = max(eps*eps_decay, eps_min)\n",
    "\n",
    "        # checkpoints @ every 3k episodes\n",
    "        if episode % checkpoint_inter ==0:\n",
    "\n",
    "            str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "            path = '/media/nero-ia/ADATA UFD/sim_data/wsh_'+str(laser_scan_state_type_atual)+str(n_sectors)+'_'+str_hora_inicio_treino\n",
    "\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            filename=path+'/wsh_'+str(laser_scan_state_type_atual)+str(n_sectors)+'_'+str_hora_inicio_treino+'checkpointn_'+str(episode)+'.out'\n",
    "            my_shelf = shelve.open(filename,flag = 'n') # 'n' for new\n",
    "\n",
    "            try:    my_shelf['hist_dict'] = hist_dict\n",
    "            except:\n",
    "                # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "                print('ERROR shelving: {0}'.format('hist_dict'))\n",
    "\n",
    "            my_shelf.close()\n",
    "\n",
    "            print('checkpoint successfull @'+str_hora_agr)\n",
    "\n",
    "\n",
    "\n",
    "    return Q_1, performance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "def get_complete_Q_1_from_shelve(workspace_folder_path, agg_method, n_sectors):\n",
    "    myshelve = shelve.open(workspace_folder_path+'/wsh_{metodo}{n_setores}completo.out'.format(\n",
    "        metodo = agg_method,n_setores = n_sectors\n",
    "    ))\n",
    "    Q_1 = myshelve['Q_1']\n",
    "    myshelve.close()\n",
    "    return Q_1\n",
    "\n",
    "dict_Q_1 = {}\n",
    "for agg_method, n_sectors in training_list:\n",
    "    print('Processing: {agg_method}-{n_sectors}'.format(agg_method=agg_method,  n_sectors=n_sectors))\n",
    "    current_Q_1 = get_complete_Q_1_from_shelve(workspace_folder_path, agg_method, n_sectors)\n",
    "\n",
    "    dict_Q_1[agg_method+'_'+n_sectors] = current_Q_1\n",
    "\n",
    "    del current_Q_1\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open('q1nets_25k.pickle', 'wb') as handle:\n",
    "    pickle.dump(dict_Q_1, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "workspace_folder_path = '/media/nero-ia/7A309A87309A49D1/sia_23/25k/'\n",
    "\n",
    "training_list = [\n",
    "    ['mean','4']\n",
    "    ,['mean','5']\n",
    "    ,['mean','6']\n",
    "    ,['mean','10']\n",
    "    ,['min','4']\n",
    "    ,['min','5']\n",
    "    ,['min','6']\n",
    "    ,['min','10']\n",
    "    ,['mode','4']\n",
    "    ,['mode','5']\n",
    "    ,['mode','6']\n",
    "    ,['mode','10']\n",
    "    ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/nero-ia/Documents/Caldeira/sia_DRL_2023/retrieve_and_save_q1.ipynb Cell 4\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nero-ia/Documents/Caldeira/sia_DRL_2023/retrieve_and_save_q1.ipynb#W3sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpickle\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/nero-ia/Documents/Caldeira/sia_DRL_2023/retrieve_and_save_q1.ipynb#W3sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mq1nets_25k.pickle\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/nero-ia/Documents/Caldeira/sia_DRL_2023/retrieve_and_save_q1.ipynb#W3sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     dict_Q_1 \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39;49mload(f)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/storage.py:240\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    239\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 240\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/serialization.py:795\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    793\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    794\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 795\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/serialization.py:1012\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1010\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1011\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1012\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1014\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1016\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/serialization.py:958\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    954\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    955\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    956\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    957\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 958\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    959\u001b[0m         dtype\u001b[39m=\u001b[39mdtype)\n\u001b[1;32m    961\u001b[0m typed_storage \u001b[39m=\u001b[39m deserialized_objects[root_key]\n\u001b[1;32m    962\u001b[0m \u001b[39mif\u001b[39;00m view_metadata \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/serialization.py:215\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 215\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    216\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    217\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/tf-gpu/lib/python3.9/site-packages/torch/serialization.py:185\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m     \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n\u001b[0;32m--> 185\u001b[0m         \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mUntypedStorage(obj\u001b[39m.\u001b[39;49mnbytes(), device\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(location))\n\u001b[1;32m    186\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39mcuda(device)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."
     ]
    }
   ],
   "source": [
    "#for agg_method, n_sectors in training_list:\n",
    "import pickle\n",
    "with open('q1nets_25k.pickle', 'rb') as f:\n",
    "    dict_Q_1 = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for method, q_network in dict_Q_1.items():\n",
    "    for current_goal in [3]:\n",
    "        print(method.split('_'))\n",
    "\n",
    "\n",
    "        rospy.init_node('learning_loop')\n",
    "        rospy.wait_for_service('/gazebo/reset_world')\n",
    "        pause_physics_client=rospy.ServiceProxy('/gazebo/pause_physics',Empty)\n",
    "        unpause_physics_client=rospy.ServiceProxy('/gazebo/unpause_physics',Empty)\n",
    "        reset_simulation = rospy.ServiceProxy('/gazebo/reset_simulation', Empty)\n",
    "\n",
    "        #Initialize relevant objects\n",
    "        Agent=AgentClass()\n",
    "        Com=CommunicationP3DX(Agent)\n",
    "\n",
    "        Agent.get_state_discrete()\n",
    "        Agent.get_reward(1)\n",
    "        rospy.sleep(1)\n",
    "        Agent.get_state_discrete()\n",
    "        Agent.get_reward(1)\n",
    "        rospy.sleep(1)\n",
    "        Agent.get_state_discrete()\n",
    "        Agent.get_reward(1)\n",
    "        reset_simulation()\n",
    "        Agent.goal = {0:[-8,8],\n",
    "                    1:[-2,8],\n",
    "                    2:[8,-3],\n",
    "                    3:[-8,-8],\n",
    "                    4:[8,8]}[current_goal]\n",
    "\n",
    "        starting_pos = {\n",
    "                0:[-4,2,[0,0,0,0]],\n",
    "                1:[-4,2,[0,0,0,0]],\n",
    "                2:[0,-5,[0,0,0,0]],\n",
    "                3:[0,-5,[0,0,0,0]],\n",
    "                4:[3,2,[0,0,0,0]]\n",
    "        }[current_goal]\n",
    "\n",
    "        for _ in [1,2]:\n",
    "            Com.StateDef(x = starting_pos[0],y =starting_pos[1], oz=starting_pos[2])\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "        if len(Agent.state)>0 and len(Agent.past_state)>0:\n",
    "            rospy.loginfo('States Initialized Succesfuly')\n",
    "            print('Current goal ->' +str(Agent.goal))\n",
    "        else:\n",
    "            rospy.loginfo('States not received!!')\n",
    "\n",
    "        laser_scan_state_type_atual = method.split('_')[0]\n",
    "        theta_atual = {\n",
    "                '4':46,\n",
    "                '5':36.1,\n",
    "                '6':30.1,\n",
    "                '10': 18.1\n",
    "            }[method.split('_')[1]]\n",
    "\n",
    "        horizon_eval = 60\n",
    "        repeats = 10#1000\n",
    "\n",
    "        q_network.eval()\n",
    "        perform = 0\n",
    "        for repeat_i in range(repeats):\n",
    "\n",
    "            #env.reset()\n",
    "            #print(theta_atual)\n",
    "            state = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            reset_simulation()\n",
    "            for _ in [1,2]:\n",
    "                Com.StateDef(x = starting_pos[0],y =starting_pos[1], oz=starting_pos[2])\n",
    "\n",
    "\n",
    "            done = False\n",
    "            i= 0\n",
    "            while not done:\n",
    "                i += 1\n",
    "\n",
    "                state = torch.Tensor(state).to(device)\n",
    "                with torch.no_grad():\n",
    "                    values = q_network(state)\n",
    "                action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "                if action==0:\n",
    "                    Com.action_forward()\n",
    "                elif action==1:\n",
    "                    Com.action_right()\n",
    "                else:\n",
    "                    Com.action_left()\n",
    "\n",
    "                state=Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "                reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "                done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon_eval, reach_dist=0.5)\n",
    "\n",
    "                if i > horizon_eval:\n",
    "                    print(method)\n",
    "                    print([reward,new_distance,status_done])\n",
    "                    done = True\n",
    "\n",
    "                perform += reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'v': {'b': [1, 2, 2]}}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = {}\n",
    "a['v'] = {'b':[1,2,2]}\n",
    "\n",
    "b = a.copy()\n",
    "b"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
