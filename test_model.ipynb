{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent\n",
      "AgentClass\n",
      "CommunicationP3DX\n",
      "Empty\n",
      "Memory\n",
      "QNetwork\n",
      "QNetworkCNN\n",
      "Q_1\n",
      "StepLR\n",
      "__cached__\n",
      "__doc__\n",
      "__file__\n",
      "__loader__\n",
      "__name__\n",
      "__package__\n",
      "__spec__\n",
      "__warningregistry__\n",
      "actions_angular\n",
      "actions_linear\n",
      "datetime\n",
      "device\n",
      "evaluate\n",
      "filename\n",
      "goal_zones_x\n",
      "goal_zones_y\n",
      "hist_dict\n",
      "key\n",
      "laser_scan_state_type_atual\n",
      "main\n",
      "my_shelf\n",
      "n_sectors\n",
      "path\n",
      "pause_physics_client\n",
      "performance\n",
      "reset_simulation\n",
      "select_action\n",
      "str_hora_agr\n",
      "str_hora_inicio_treino\n",
      "theta_atual\n",
      "train\n",
      "unpause_physics_client\n",
      "update_parameters\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from CommunicationP3DX import CommunicationP3DX\n",
    "from Agent import AgentClass\n",
    "# from std_srvs.srv import Empty\n",
    "import rospy\n",
    "import time\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def get_complete_hist_from_shelve(workspace_folder_path, agg_method, n_sectors):\n",
    "    return shelve.open(workspace_folder_path+'/wsh_{metodo}{n_setores}completo.out'.format(\n",
    "        metodo = agg_method,n_setores = n_sectors\n",
    "    ))['hist_dict']\n",
    "\n",
    "# training_list = [\n",
    "#     ['mean','4']\n",
    "#     ,['mean','5']\n",
    "#     ,['mean','6']\n",
    "#     ,['mean','10']\n",
    "#     ,['min','4']\n",
    "#     ,['min','5']\n",
    "#     ,['min','6']\n",
    "#     ,['min','10']\n",
    "#     ]\n",
    "\n",
    "# get hists by method\n",
    "workspace_folder_path = '/media/xnd/7A309A87309A49D1/sia_23/25k/'\n",
    "#  'C:/Users/alexa/Desktop/sia_DRL_2023/25k/'\n",
    "\n",
    "agg_method = 'min'\n",
    "n_sectors = 10\n",
    "\n",
    "workspace = shelve.open(workspace_folder_path+'/wsh_{metodo}{n_setores}completo.out'.format(\n",
    "        metodo = agg_method,n_setores = n_sectors\n",
    "    ))\n",
    "\n",
    "vars = list(workspace.keys())\n",
    "for var in vars: print(var)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\"\"\"\n",
    "Adapted from https://github.com/fschur/DDQN-with-PyTorch-for-OpenAI-Gym/blob/master/DDQN_discrete.py#L21\n",
    "for a gazebo application.\n",
    "\n",
    "Implementation of Double DQN for gym environments with discrete action space.\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from CommunicationP3DX import CommunicationP3DX\n",
    "from Agent import AgentClass\n",
    "from std_srvs.srv import Empty\n",
    "import rospy\n",
    "import time\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "str_hora_inicio_treino = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "\n",
    "\"\"\"\n",
    "The Q-Network has as input a state s and outputs the state-action values q(s,a_1), ..., q(s,a_n) for all n actions.\n",
    "\"\"\"\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        x1 = F.leaky_relu(self.fc_1(inp))\n",
    "        x1 = F.leaky_relu(self.fc_2(x1))\n",
    "        x1 = self.fc_3(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If the observations are images we use CNNs.\n",
    "\"\"\"\n",
    "class QNetworkCNN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(QNetworkCNN, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, kernel_size=4, stride=3)\n",
    "        self.conv_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc_1 = nn.Linear(8960, 512)\n",
    "        self.fc_2 = nn.Linear(512, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view((1, 3, 210, 160))\n",
    "        x1 = F.relu(self.conv_1(inp))\n",
    "        x1 = F.relu(self.conv_2(x1))\n",
    "        x1 = F.relu(self.conv_3(x1))\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x1 = F.leaky_relu(self.fc_1(x1))\n",
    "        x1 = self.fc_2(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "memory to save the state, action, reward sequence from the current episode.\n",
    "\"\"\"\n",
    "class Memory:\n",
    "    def __init__(self, len):\n",
    "        self.rewards = collections.deque(maxlen=len)\n",
    "        self.state = collections.deque(maxlen=len)\n",
    "        self.action = collections.deque(maxlen=len)\n",
    "        self.is_done = collections.deque(maxlen=len)\n",
    "\n",
    "    def update(self, state, action, reward, done):\n",
    "        # if the episode is finished we do not save to new state. Otherwise we have more states per episode than rewards\n",
    "        # and actions whcih leads to a mismatch when we sample from memory.\n",
    "        if not done:\n",
    "            self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.is_done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        sample \"batch_size\" many (state, action, reward, next state, is_done) datapoints.\n",
    "        \"\"\"\n",
    "        n = len(self.is_done)\n",
    "        idx = random.sample(range(0, n-1), batch_size)\n",
    "\n",
    "        return torch.Tensor(self.state)[idx].to(device), torch.LongTensor(self.action)[idx].to(device), \\\n",
    "               torch.Tensor(self.state)[1+np.array(idx)].to(device), torch.Tensor(self.rewards)[idx].to(device), \\\n",
    "               torch.Tensor(self.is_done)[idx].to(device)\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards.clear()\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.is_done.clear()\n",
    "\n",
    "\n",
    "def select_action(model, action_space_size, state, eps):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    with torch.no_grad():\n",
    "        values = model(state)\n",
    "\n",
    "    # select a random action wih probability eps\n",
    "    if random.random() <= eps:\n",
    "        action = np.random.randint(0, action_space_size)\n",
    "    else:\n",
    "        action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def train(batch_size, current, target, optim, memory, gamma):\n",
    "\n",
    "    states, actions, next_states, rewards, is_done = memory.sample(batch_size)\n",
    "\n",
    "    q_values = current(states)\n",
    "\n",
    "    next_q_values = current(next_states)\n",
    "    next_q_state_values = target(next_states)\n",
    "\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = rewards + gamma * next_q_value * (1 - is_done)\n",
    "\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "def evaluate(Qmodel, horizon_eval, repeats,episode):\n",
    "    \"\"\"\n",
    "    Runs a greedy policy with respect to the current Q-Network for \"repeats\" many episodes. Returns the average\n",
    "    episode reward.\n",
    "    \"\"\"\n",
    "    Qmodel.eval()\n",
    "    perform = 0\n",
    "    for repeat_i in range(repeats):\n",
    "\n",
    "        #env.reset()\n",
    "        state = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "        reset_simulation()\n",
    "\n",
    "        done = False\n",
    "        i= 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                values = Qmodel(state)\n",
    "            action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "            if action==0:\n",
    "                Com.action_forward()\n",
    "            elif action==1:\n",
    "                Com.action_right()\n",
    "            else:\n",
    "                Com.action_left()\n",
    "\n",
    "            state=Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "            done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon_eval, reach_dist=0.5)\n",
    "\n",
    "            if i > horizon_eval:\n",
    "                done = True\n",
    "\n",
    "        if repeat_i==0:\n",
    "            hist_dict['rewards'][episode+1] = [[reward, new_distance, r1, r4]]\n",
    "            hist_dict['epresult'][episode+1] = [done, status_done]\n",
    "\n",
    "            hist_dict['pos_eval'][episode+1] = [Agent.Pos]\n",
    "            hist_dict['scan_eval'][episode+1] = [Agent.laser_scan]\n",
    "            hist_dict['state_eval'][episode+1] = [state]\n",
    "        else:\n",
    "            hist_dict['rewards'][episode+1].append([reward, new_distance, r1, r4])\n",
    "            hist_dict['epresult'][episode+1].append([done, status_done])\n",
    "\n",
    "            hist_dict['pos_eval'][episode+1].append(Agent.Pos)\n",
    "            hist_dict['scan_eval'][episode+1].append(Agent.laser_scan)\n",
    "            hist_dict['state_eval'][episode+1].append(state)\n",
    "\n",
    "\n",
    "            perform += reward\n",
    "    Qmodel.train()\n",
    "    return perform/repeats, new_distance, r1, r4\n",
    "\n",
    "\n",
    "def update_parameters(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "\n",
    "def main(state_space_size, action_space_size=3, gamma=0.99, lr=1e-3, min_episodes=20, eps=1, eps_decay=0.995,\n",
    "         eps_min=0.01, update_step=10, batch_size=64, update_repeats=50, num_episodes=3000, max_memory_size=50000,\n",
    "         lr_gamma=0.9, lr_step=100, measure_step=100, measure_repeats=100, hidden_dim=64, cnn=False, horizon=200000,horizon_eval=50,\n",
    "         theta_atual:int= 36,laser_scan_state_type_atual:str = 'min',checkpoint_inter=1000):\n",
    "    \"\"\"\n",
    "    :param gamma: reward discount factor\n",
    "    :param lr: learning rate for the Q-Network\n",
    "    :param min_episodes: we wait \"min_episodes\" many episodes in order to aggregate enough data before starting to train\n",
    "    :param eps: probability to take a random action during training\n",
    "    :param eps_decay: after every episode \"eps\" is multiplied by \"eps_decay\" to reduces exploration over time\n",
    "    :param eps_min: minimal value of \"eps\"\n",
    "    :param update_step: after \"update_step\" many episodes the Q-Network is trained \"update_repeats\" many times with a\n",
    "    batch of size \"batch_size\" from the memory.\n",
    "    :param batch_size: see above\n",
    "    :param update_repeats: see above\n",
    "    :param num_episodes: the number of episodes played in total\n",
    "    :param seed: random seed for reproducibility\n",
    "    :param max_memory_size: size of the replay memory\n",
    "    :param lr_gamma: learning rate decay for the Q-Network\n",
    "    :param lr_step: every \"lr_step\" episodes we decay the learning rate\n",
    "    :param measure_step: every \"measure_step\" episode the performance is measured\n",
    "    :param measure_repeats: the amount of episodes played in to asses performance\n",
    "    :param hidden_dim: hidden dimensions for the Q_network\n",
    "    :param env_name: name of the gym environment\n",
    "    :param cnn: set to \"True\" when using environments with image observations like \"Pong-v0\"\n",
    "    :param horizon: number of steps taken in the environment before terminating the episode (prevents very long episodes)\n",
    "    :param render: if \"True\" renders the environment every \"render_step\" episodes\n",
    "    :param render_step: see above\n",
    "    :return: the trained Q-Network and the measured performances\n",
    "    \"\"\"\n",
    "\n",
    "    if cnn:\n",
    "        Q_1 = QNetworkCNN(action_dim=action_space_size).to(device)\n",
    "        Q_2 = QNetworkCNN(action_dim=action_space_size).to(device)\n",
    "    else:\n",
    "        Q_1 = QNetwork(action_dim=action_space_size, state_dim=state_space_size,\n",
    "                                        hidden_dim=hidden_dim).to(device)\n",
    "        Q_2 = QNetwork(action_dim=action_space_size, state_dim=state_space_size,\n",
    "                                        hidden_dim=hidden_dim).to(device)\n",
    "    # transfer parameters from Q_1 to Q_2\n",
    "    update_parameters(Q_1, Q_2)\n",
    "\n",
    "    # we only train Q_1\n",
    "    for param in Q_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # https://douglasrizzo.com.br/step-decay-lr/\n",
    "    initial_lr = lr\n",
    "    final_lr = lr*1e-3\n",
    "    n_updates = num_episodes*0.7\n",
    "    lr_gamma =  (final_lr / initial_lr)**(1 / n_updates) # gamma\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q_1.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "\n",
    "    memory = Memory(max_memory_size)\n",
    "    performance = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # display the performance\n",
    "        if (episode % measure_step == 0) and episode >= min_episodes:\n",
    "            print('eval...')\n",
    "            reward_eval, new_distance, r1, r4 = evaluate(Q_1, horizon_eval, measure_repeats,episode)\n",
    "            performance.append([episode, reward_eval])\n",
    "            print(\"Episode: \", episode)\n",
    "            print(\"rewards: \", performance[-1][1])\n",
    "            print(\"lr: \", scheduler.get_lr()[0])\n",
    "            print(\"eps: \", eps)\n",
    "\n",
    "            # hist_dict = {'pos':{}, 'scan':{}, 'rewards':{}, 'rates':{}, 'state':{}, 'epresult':{}}\n",
    "            hist_dict['rewards_eval'][episode+1] = [performance[-1][1], new_distance, r1, r4]\n",
    "            hist_dict['rates'][episode+1] = [eps, scheduler.get_lr()[0]]\n",
    "\n",
    "        reset_simulation() #env.reset()\n",
    "        Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "        state = Agent.state\n",
    "        #print(state)\n",
    "        memory.state.append(state)\n",
    "\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "            #if state is None: break\n",
    "            action = select_action(Q_2, action_space_size, state, eps)\n",
    "\n",
    "            #TODO take action here, calculate rewards, check if done, refresh state variable\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if action==0:\n",
    "                Com.action_forward()\n",
    "            elif action==1:\n",
    "                Com.action_right()\n",
    "            else:\n",
    "                Com.action_left()\n",
    "\n",
    "\n",
    "            state  = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            #print(state)\n",
    "            #reward, _, _, _ = Agent.get_reward(number_iterations=i)\n",
    "            reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "            done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon, reach_dist=0.5)\n",
    "\n",
    "\n",
    "            # hist_dict = {'pos':{}, 'scan':{}, 'rewards':{}, 'rates':{}, 'state':{}, 'epresult':{}}\n",
    "            if i==1:\n",
    "                hist_dict['pos'][episode+1] = [Agent.Pos]\n",
    "                hist_dict['scan'][episode+1] = [Agent.laser_scan]\n",
    "                hist_dict['state'][episode+1] = [state]\n",
    "            else:\n",
    "                hist_dict['pos'][episode+1].append(Agent.Pos)\n",
    "                hist_dict['scan'][episode+1].append(Agent.laser_scan)\n",
    "                hist_dict['state'][episode+1].append(state)\n",
    "\n",
    "            if i > horizon:\n",
    "                done = True\n",
    "\n",
    "            if (episode % (measure_step) == 0) and i%5==0:\n",
    "                if i<=5:\n",
    "                    str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "                    print('Epsisode '+str(episode)+' @'+str_hora_agr)\n",
    "\n",
    "                str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "                print('i: '+str(i)+' \\tr:'+str([reward, new_distance, r1, r4]))\n",
    "\n",
    "            # save state, action, reward sequence\n",
    "            memory.update(state, action, reward, done)\n",
    "\n",
    "\n",
    "        if episode >= min_episodes and episode % update_step == 0:\n",
    "            for _ in range(update_repeats):\n",
    "\n",
    "                pause_physics_client() # pause simulation to train\n",
    "                train(batch_size, Q_1, Q_2, optimizer, memory, gamma)\n",
    "                unpause_physics_client() # unpause simulation after training\n",
    "                reset_simulation()\n",
    "\n",
    "            # transfer new parameter from Q_1 to Q_2\n",
    "            update_parameters(Q_1, Q_2)\n",
    "            # print(Agent.goal)\n",
    "\n",
    "        # update learning rate and eps\n",
    "        scheduler.step()\n",
    "        eps = max(eps*eps_decay, eps_min)\n",
    "\n",
    "        # checkpoints @ every 3k episodes\n",
    "        if episode % checkpoint_inter ==0:\n",
    "\n",
    "            str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "            path = '/media/nero-ia/ADATA UFD/sim_data/wsh_'+str(laser_scan_state_type_atual)+str(n_sectors)+'_'+str_hora_inicio_treino\n",
    "\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            filename=path+'/wsh_'+str(laser_scan_state_type_atual)+str(n_sectors)+'_'+str_hora_inicio_treino+'checkpointn_'+str(episode)+'.out'\n",
    "            my_shelf = shelve.open(filename,flag = 'n') # 'n' for new\n",
    "\n",
    "            try:    my_shelf['hist_dict'] = hist_dict\n",
    "            except:\n",
    "                # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "                print('ERROR shelving: {0}'.format('hist_dict'))\n",
    "\n",
    "            my_shelf.close()\n",
    "            \n",
    "            print('checkpoint successfull @'+str_hora_agr)\n",
    "\n",
    "\n",
    "\n",
    "    return Q_1, performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/shelve.py:111\u001b[0m, in \u001b[0;36mShelf.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m     value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache[key]\n\u001b[1;32m    112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Q_1'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m path1 \u001b[39m=\u001b[39m workspace_folder_path\u001b[39m+\u001b[39m\u001b[39m'\u001b[39m\u001b[39m/wsh_\u001b[39m\u001b[39m{metodo}\u001b[39;00m\u001b[39m{n_setores}\u001b[39;00m\u001b[39mcompleto.out\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         metodo \u001b[39m=\u001b[39m agg_method,n_setores \u001b[39m=\u001b[39m n_sectors\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39m# qn.load_state_dict(torch.load(shelve.open(path1))) \u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m qn \u001b[39m=\u001b[39m my_shelf[\u001b[39m'\u001b[39;49m\u001b[39mQ_1\u001b[39;49m\u001b[39m'\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39mprint\u001b[39m(my_shelf[\u001b[39m'\u001b[39m\u001b[39mQ_1\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/xnd/Desktop/sia_DRL_2023/test_model.ipynb#W2sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m my_shelf\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/shelve.py:114\u001b[0m, in \u001b[0;36mShelf.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m     f \u001b[39m=\u001b[39m BytesIO(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdict[key\u001b[39m.\u001b[39mencode(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeyencoding)])\n\u001b[0;32m--> 114\u001b[0m     value \u001b[39m=\u001b[39m Unpickler(f)\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    115\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriteback:\n\u001b[1;32m    116\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache[key] \u001b[39m=\u001b[39m value\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/storage.py:241\u001b[0m, in \u001b[0;36m_load_from_bytes\u001b[0;34m(b)\u001b[0m\n\u001b[1;32m    240\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_load_from_bytes\u001b[39m(b):\n\u001b[0;32m--> 241\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mload(io\u001b[39m.\u001b[39;49mBytesIO(b))\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[39mraise\u001b[39;00m pickle\u001b[39m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[39m+\u001b[39m \u001b[39mstr\u001b[39m(e)) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m--> 815\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1041\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1042\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m-> 1043\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m   1045\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1047\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    976\u001b[0m     obj\u001b[39m.\u001b[39m_torch_load_uninitialized \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[39m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     \u001b[39m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     typed_storage \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mstorage\u001b[39m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 980\u001b[0m         wrap_storage\u001b[39m=\u001b[39mrestore_location(obj, location),\n\u001b[1;32m    981\u001b[0m         dtype\u001b[39m=\u001b[39mdtype,\n\u001b[1;32m    982\u001b[0m         _internal\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[39m=\u001b[39m typed_storage\n\u001b[1;32m    984\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[39mfor\u001b[39;00m _, _, fn \u001b[39min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[39m=\u001b[39m fn(storage, location)\n\u001b[1;32m    218\u001b[0m         \u001b[39mif\u001b[39;00m result \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[39mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/serialization.py:182\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    181\u001b[0m     \u001b[39mif\u001b[39;00m location\u001b[39m.\u001b[39mstartswith(\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m--> 182\u001b[0m         device \u001b[39m=\u001b[39m validate_cuda_device(location)\n\u001b[1;32m    183\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(obj, \u001b[39m\"\u001b[39m\u001b[39m_torch_load_uninitialized\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m             \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/miniconda3/envs/DRL/lib/python3.8/site-packages/torch/serialization.py:166\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    163\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39m_utils\u001b[39m.\u001b[39m_get_device_index(location, \u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available():\n\u001b[0;32m--> 166\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mAttempting to deserialize object on a CUDA \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    167\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    168\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mIf you are running on a CPU-only machine, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    169\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39mcpu\u001b[39m\u001b[39m\\'\u001b[39;00m\u001b[39m) \u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    170\u001b[0m                        \u001b[39m'\u001b[39m\u001b[39mto map your storages to the CPU.\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m    171\u001b[0m device_count \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mdevice_count()\n\u001b[1;32m    172\u001b[0m \u001b[39mif\u001b[39;00m device \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "map_location=torch.device('cpu') \n",
    "\n",
    "qn = QNetworkCNN(action_dim=11)\n",
    "\n",
    "my_shelf = shelve.open(workspace_folder_path+'/wsh_{metodo}{n_setores}completo.out'.format(\n",
    "        metodo = agg_method,n_setores = n_sectors\n",
    "    ))\n",
    "    \n",
    "# for key in my_shelf:\n",
    "    # globals()[key]=my_shelf[key]\n",
    "# key = 'Q_1'\n",
    "# globals()[key]=my_shelf[key]\n",
    "\n",
    "\n",
    "path1 = workspace_folder_path+'/wsh_{metodo}{n_setores}completo.out'.format(\n",
    "        metodo = agg_method,n_setores = n_sectors\n",
    "    )\n",
    "\n",
    "qn = my_shelf['Q_1']\n",
    "\n",
    "# qn.load_state_dict(torch.load(shelve.open(path1))) \n",
    "\n",
    "print(my_shelf['Q_1'])\n",
    "my_shelf.close()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DRL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
