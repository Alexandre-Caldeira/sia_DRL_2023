{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shelve\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "import random\n",
    "import torch.nn.functional as F\n",
    "import collections\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\"\"\"\n",
    "Adapted from https://github.com/fschur/DDQN-with-PyTorch-for-OpenAI-Gym/blob/master/DDQN_discrete.py#L21\n",
    "for a gazebo application.\n",
    "\n",
    "Implementation of Double DQN for gym environments with discrete action space.\n",
    "\"\"\"\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "from CommunicationP3DX import CommunicationP3DX\n",
    "from Agent import AgentClass\n",
    "from std_srvs.srv import Empty\n",
    "import rospy\n",
    "import time\n",
    "import shelve\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "str_hora_inicio_treino = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "\n",
    "\"\"\"\n",
    "The Q-Network has as input a state s and outputs the state-action values q(s,a_1), ..., q(s,a_n) for all n actions.\n",
    "\"\"\"\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, action_dim, state_dim, hidden_dim):\n",
    "        super(QNetwork, self).__init__()\n",
    "\n",
    "        self.fc_1 = nn.Linear(state_dim, hidden_dim)\n",
    "        self.fc_2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc_3 = nn.Linear(hidden_dim, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "\n",
    "        x1 = F.leaky_relu(self.fc_1(inp))\n",
    "        x1 = F.leaky_relu(self.fc_2(x1))\n",
    "        x1 = self.fc_3(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "If the observations are images we use CNNs.\n",
    "\"\"\"\n",
    "class QNetworkCNN(nn.Module):\n",
    "    def __init__(self, action_dim):\n",
    "        super(QNetworkCNN, self).__init__()\n",
    "\n",
    "        self.conv_1 = nn.Conv2d(3, 32, kernel_size=8, stride=4)\n",
    "        self.conv_2 = nn.Conv2d(32, 64, kernel_size=4, stride=3)\n",
    "        self.conv_3 = nn.Conv2d(64, 64, kernel_size=3, stride=1)\n",
    "        self.fc_1 = nn.Linear(8960, 512)\n",
    "        self.fc_2 = nn.Linear(512, action_dim)\n",
    "\n",
    "    def forward(self, inp):\n",
    "        inp = inp.view((1, 3, 210, 160))\n",
    "        x1 = F.relu(self.conv_1(inp))\n",
    "        x1 = F.relu(self.conv_2(x1))\n",
    "        x1 = F.relu(self.conv_3(x1))\n",
    "        x1 = torch.flatten(x1, 1)\n",
    "        x1 = F.leaky_relu(self.fc_1(x1))\n",
    "        x1 = self.fc_2(x1)\n",
    "\n",
    "        return x1\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "memory to save the state, action, reward sequence from the current episode.\n",
    "\"\"\"\n",
    "class Memory:\n",
    "    def __init__(self, len):\n",
    "        self.rewards = collections.deque(maxlen=len)\n",
    "        self.state = collections.deque(maxlen=len)\n",
    "        self.action = collections.deque(maxlen=len)\n",
    "        self.is_done = collections.deque(maxlen=len)\n",
    "\n",
    "    def update(self, state, action, reward, done):\n",
    "        # if the episode is finished we do not save to new state. Otherwise we have more states per episode than rewards\n",
    "        # and actions whcih leads to a mismatch when we sample from memory.\n",
    "        if not done:\n",
    "            self.state.append(state)\n",
    "        self.action.append(action)\n",
    "        self.rewards.append(reward)\n",
    "        self.is_done.append(done)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        sample \"batch_size\" many (state, action, reward, next state, is_done) datapoints.\n",
    "        \"\"\"\n",
    "        n = len(self.is_done)\n",
    "        idx = random.sample(range(0, n-1), batch_size)\n",
    "\n",
    "        return torch.Tensor(self.state)[idx].to(device), torch.LongTensor(self.action)[idx].to(device), \\\n",
    "               torch.Tensor(self.state)[1+np.array(idx)].to(device), torch.Tensor(self.rewards)[idx].to(device), \\\n",
    "               torch.Tensor(self.is_done)[idx].to(device)\n",
    "\n",
    "    def reset(self):\n",
    "        self.rewards.clear()\n",
    "        self.state.clear()\n",
    "        self.action.clear()\n",
    "        self.is_done.clear()\n",
    "\n",
    "\n",
    "def select_action(model, action_space_size, state, eps):\n",
    "    state = torch.Tensor(state).to(device)\n",
    "    with torch.no_grad():\n",
    "        values = model(state)\n",
    "\n",
    "    # select a random action wih probability eps\n",
    "    if random.random() <= eps:\n",
    "        action = np.random.randint(0, action_space_size)\n",
    "    else:\n",
    "        action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "    return action\n",
    "\n",
    "\n",
    "def train(batch_size, current, target, optim, memory, gamma):\n",
    "\n",
    "    states, actions, next_states, rewards, is_done = memory.sample(batch_size)\n",
    "\n",
    "    q_values = current(states)\n",
    "\n",
    "    next_q_values = current(next_states)\n",
    "    next_q_state_values = target(next_states)\n",
    "\n",
    "    q_value = q_values.gather(1, actions.unsqueeze(1)).squeeze(1)\n",
    "    next_q_value = next_q_state_values.gather(1, torch.max(next_q_values, 1)[1].unsqueeze(1)).squeeze(1)\n",
    "    expected_q_value = rewards + gamma * next_q_value * (1 - is_done)\n",
    "\n",
    "    loss = (q_value - expected_q_value.detach()).pow(2).mean()\n",
    "\n",
    "    optim.zero_grad()\n",
    "    loss.backward()\n",
    "    optim.step()\n",
    "\n",
    "def evaluate(Qmodel, horizon_eval, repeats,episode):\n",
    "    \"\"\"\n",
    "    Runs a greedy policy with respect to the current Q-Network for \"repeats\" many episodes. Returns the average\n",
    "    episode reward.\n",
    "    \"\"\"\n",
    "    Qmodel.eval()\n",
    "    perform = 0\n",
    "    for repeat_i in range(repeats):\n",
    "\n",
    "        #env.reset()\n",
    "        state = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "        reset_simulation()\n",
    "\n",
    "        done = False\n",
    "        i= 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "\n",
    "            state = torch.Tensor(state).to(device)\n",
    "            with torch.no_grad():\n",
    "                values = Qmodel(state)\n",
    "            action = np.argmax(values.cpu().numpy())\n",
    "\n",
    "            if action==0:\n",
    "                Com.action_forward()\n",
    "            elif action==1:\n",
    "                Com.action_right()\n",
    "            else:\n",
    "                Com.action_left()\n",
    "\n",
    "            state=Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "            done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon_eval, reach_dist=0.5)\n",
    "\n",
    "            if i > horizon_eval:\n",
    "                done = True\n",
    "\n",
    "        if repeat_i==0:\n",
    "            hist_dict['rewards'][episode+1] = [[reward, new_distance, r1, r4]]\n",
    "            hist_dict['epresult'][episode+1] = [done, status_done]\n",
    "\n",
    "            hist_dict['pos_eval'][episode+1] = [Agent.Pos]\n",
    "            hist_dict['scan_eval'][episode+1] = [Agent.laser_scan]\n",
    "            hist_dict['state_eval'][episode+1] = [state]\n",
    "        else:\n",
    "            hist_dict['rewards'][episode+1].append([reward, new_distance, r1, r4])\n",
    "            hist_dict['epresult'][episode+1].append([done, status_done])\n",
    "\n",
    "            hist_dict['pos_eval'][episode+1].append(Agent.Pos)\n",
    "            hist_dict['scan_eval'][episode+1].append(Agent.laser_scan)\n",
    "            hist_dict['state_eval'][episode+1].append(state)\n",
    "\n",
    "\n",
    "            perform += reward\n",
    "    Qmodel.train()\n",
    "    return perform/repeats, new_distance, r1, r4\n",
    "\n",
    "\n",
    "def update_parameters(current_model, target_model):\n",
    "    target_model.load_state_dict(current_model.state_dict())\n",
    "\n",
    "\n",
    "def main(state_space_size, action_space_size=3, gamma=0.99, lr=1e-3, min_episodes=20, eps=1, eps_decay=0.995,\n",
    "         eps_min=0.01, update_step=10, batch_size=64, update_repeats=50, num_episodes=3000, max_memory_size=50000,\n",
    "         lr_gamma=0.9, lr_step=100, measure_step=100, measure_repeats=100, hidden_dim=64, cnn=False, horizon=200000,horizon_eval=50,\n",
    "         theta_atual:int= 36,laser_scan_state_type_atual:str = 'min',checkpoint_inter=1000):\n",
    "    \"\"\"\n",
    "    :param gamma: reward discount factor\n",
    "    :param lr: learning rate for the Q-Network\n",
    "    :param min_episodes: we wait \"min_episodes\" many episodes in order to aggregate enough data before starting to train\n",
    "    :param eps: probability to take a random action during training\n",
    "    :param eps_decay: after every episode \"eps\" is multiplied by \"eps_decay\" to reduces exploration over time\n",
    "    :param eps_min: minimal value of \"eps\"\n",
    "    :param update_step: after \"update_step\" many episodes the Q-Network is trained \"update_repeats\" many times with a\n",
    "    batch of size \"batch_size\" from the memory.\n",
    "    :param batch_size: see above\n",
    "    :param update_repeats: see above\n",
    "    :param num_episodes: the number of episodes played in total\n",
    "    :param seed: random seed for reproducibility\n",
    "    :param max_memory_size: size of the replay memory\n",
    "    :param lr_gamma: learning rate decay for the Q-Network\n",
    "    :param lr_step: every \"lr_step\" episodes we decay the learning rate\n",
    "    :param measure_step: every \"measure_step\" episode the performance is measured\n",
    "    :param measure_repeats: the amount of episodes played in to asses performance\n",
    "    :param hidden_dim: hidden dimensions for the Q_network\n",
    "    :param env_name: name of the gym environment\n",
    "    :param cnn: set to \"True\" when using environments with image observations like \"Pong-v0\"\n",
    "    :param horizon: number of steps taken in the environment before terminating the episode (prevents very long episodes)\n",
    "    :param render: if \"True\" renders the environment every \"render_step\" episodes\n",
    "    :param render_step: see above\n",
    "    :return: the trained Q-Network and the measured performances\n",
    "    \"\"\"\n",
    "\n",
    "    if cnn:\n",
    "        Q_1 = QNetworkCNN(action_dim=action_space_size).to(device)\n",
    "        Q_2 = QNetworkCNN(action_dim=action_space_size).to(device)\n",
    "    else:\n",
    "        Q_1 = QNetwork(action_dim=action_space_size, state_dim=state_space_size,\n",
    "                                        hidden_dim=hidden_dim).to(device)\n",
    "        Q_2 = QNetwork(action_dim=action_space_size, state_dim=state_space_size,\n",
    "                                        hidden_dim=hidden_dim).to(device)\n",
    "    # transfer parameters from Q_1 to Q_2\n",
    "    update_parameters(Q_1, Q_2)\n",
    "\n",
    "    # we only train Q_1\n",
    "    for param in Q_2.parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "    # https://douglasrizzo.com.br/step-decay-lr/\n",
    "    initial_lr = lr\n",
    "    final_lr = lr*1e-3\n",
    "    n_updates = num_episodes*0.7\n",
    "    lr_gamma =  (final_lr / initial_lr)**(1 / n_updates) # gamma\n",
    "\n",
    "    optimizer = torch.optim.Adam(Q_1.parameters(), lr=lr)\n",
    "    scheduler = StepLR(optimizer, step_size=lr_step, gamma=lr_gamma)\n",
    "\n",
    "    memory = Memory(max_memory_size)\n",
    "    performance = []\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        # display the performance\n",
    "        if (episode % measure_step == 0) and episode >= min_episodes:\n",
    "            print('eval...')\n",
    "            reward_eval, new_distance, r1, r4 = evaluate(Q_1, horizon_eval, measure_repeats,episode)\n",
    "            performance.append([episode, reward_eval])\n",
    "            print(\"Episode: \", episode)\n",
    "            print(\"rewards: \", performance[-1][1])\n",
    "            print(\"lr: \", scheduler.get_lr()[0])\n",
    "            print(\"eps: \", eps)\n",
    "\n",
    "            # hist_dict = {'pos':{}, 'scan':{}, 'rewards':{}, 'rates':{}, 'state':{}, 'epresult':{}}\n",
    "            hist_dict['rewards_eval'][episode+1] = [performance[-1][1], new_distance, r1, r4]\n",
    "            hist_dict['rates'][episode+1] = [eps, scheduler.get_lr()[0]]\n",
    "\n",
    "        reset_simulation() #env.reset()\n",
    "        Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "        state = Agent.state\n",
    "        #print(state)\n",
    "        memory.state.append(state)\n",
    "\n",
    "        done = False\n",
    "        i = 0\n",
    "        while not done:\n",
    "            i += 1\n",
    "            #if state is None: break\n",
    "            action = select_action(Q_2, action_space_size, state, eps)\n",
    "\n",
    "            #TODO take action here, calculate rewards, check if done, refresh state variable\n",
    "            #state, reward, done, _ = env.step(action)\n",
    "\n",
    "            if action==0:\n",
    "                Com.action_forward()\n",
    "            elif action==1:\n",
    "                Com.action_right()\n",
    "            else:\n",
    "                Com.action_left()\n",
    "\n",
    "\n",
    "            state  = Agent.get_state_discrete(laser_scan_state_type=laser_scan_state_type_atual, theta=theta_atual)\n",
    "            #print(state)\n",
    "            #reward, _, _, _ = Agent.get_reward(number_iterations=i)\n",
    "            reward, new_distance, r1, r4 = Agent.get_reward(number_iterations=i)\n",
    "            done,status_done = Agent.is_done(number_iterations=i,max_iterations=horizon, reach_dist=0.5)\n",
    "\n",
    "\n",
    "            # hist_dict = {'pos':{}, 'scan':{}, 'rewards':{}, 'rates':{}, 'state':{}, 'epresult':{}}\n",
    "            if i==1:\n",
    "                hist_dict['pos'][episode+1] = [Agent.Pos]\n",
    "                hist_dict['scan'][episode+1] = [Agent.laser_scan]\n",
    "                hist_dict['state'][episode+1] = [state]\n",
    "            else:\n",
    "                hist_dict['pos'][episode+1].append(Agent.Pos)\n",
    "                hist_dict['scan'][episode+1].append(Agent.laser_scan)\n",
    "                hist_dict['state'][episode+1].append(state)\n",
    "\n",
    "            if i > horizon:\n",
    "                done = True\n",
    "\n",
    "            if (episode % (measure_step) == 0) and i%5==0:\n",
    "                if i<=5:\n",
    "                    str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "                    print('Epsisode '+str(episode)+' @'+str_hora_agr)\n",
    "\n",
    "                str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "                print('i: '+str(i)+' \\tr:'+str([reward, new_distance, r1, r4]))\n",
    "\n",
    "            # save state, action, reward sequence\n",
    "            memory.update(state, action, reward, done)\n",
    "\n",
    "\n",
    "        if episode >= min_episodes and episode % update_step == 0:\n",
    "            for _ in range(update_repeats):\n",
    "\n",
    "                pause_physics_client() # pause simulation to train\n",
    "                train(batch_size, Q_1, Q_2, optimizer, memory, gamma)\n",
    "                unpause_physics_client() # unpause simulation after training\n",
    "                reset_simulation()\n",
    "\n",
    "            # transfer new parameter from Q_1 to Q_2\n",
    "            update_parameters(Q_1, Q_2)\n",
    "            # print(Agent.goal)\n",
    "\n",
    "        # update learning rate and eps\n",
    "        scheduler.step()\n",
    "        eps = max(eps*eps_decay, eps_min)\n",
    "\n",
    "        # checkpoints @ every 3k episodes\n",
    "        if episode % checkpoint_inter ==0:\n",
    "\n",
    "            str_hora_agr = str(datetime.now()).replace(' ','_').replace(':','').replace('-','')[0:15]\n",
    "            path = '/media/nero-ia/ADATA UFD/sim_data/wsh_'+str(laser_scan_state_type_atual)+str(n_sectors)+'_'+str_hora_inicio_treino\n",
    "\n",
    "            if not os.path.exists(path):\n",
    "                os.makedirs(path)\n",
    "\n",
    "            filename=path+'/wsh_'+str(laser_scan_state_type_atual)+str(n_sectors)+'_'+str_hora_inicio_treino+'checkpointn_'+str(episode)+'.out'\n",
    "            my_shelf = shelve.open(filename,flag = 'n') # 'n' for new\n",
    "\n",
    "            try:    my_shelf['hist_dict'] = hist_dict\n",
    "            except:\n",
    "                # __builtins__, my_shelf, and imported modules can not be shelved.\n",
    "                print('ERROR shelving: {0}'.format('hist_dict'))\n",
    "\n",
    "            my_shelf.close()\n",
    "\n",
    "            print('checkpoint successfull @'+str_hora_agr)\n",
    "\n",
    "\n",
    "\n",
    "    return Q_1, performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<shelve.DbfilenameShelf object at 0x7f4aaab1ce20>\n",
      "['hist_dict', 'Q_1', 'filename', 'key', 'laser_scan_state_type_atual', 'my_shelf', 'path', 'performance', 'str_hora_agr', 'theta_atual']\n"
     ]
    }
   ],
   "source": [
    "wrk_spc = shelve.open('/media/nero-ia/ADATA UFD/sim_data/wsh_mean6_20231006_194744checkpointn_23000.out')\n",
    "\n",
    "# q1 = wrk_spc['Q1']\n",
    "vars = list(wrk_spc.keys())\n",
    "print(wrk_spc)\n",
    "print(vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (fc_1): Linear(in_features=5, out_features=64, bias=True)\n",
       "  (fc_2): Linear(in_features=64, out_features=64, bias=True)\n",
       "  (fc_3): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q1 = wrk_spc['Q_1']\n",
    "q1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
